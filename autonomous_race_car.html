<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Projects - Eric Delgado</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="icon" type="image/png" href="./Website_Pics/Favicon.png">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <style>
        body {
            margin: 0;
            padding: 0;
            font-family: Arial, sans-serif;
            background-color: #151E3D;
            color: #99a5b1;
            height: 100vh;
            overflow: hidden;
            display: flex;
            flex-direction: column;
            align-items: center;
        }
        body.alt-color-scheme {
            background-color: #E4EFF1;
            color: black;
        }
        #right-panel {
            width: 90%;
            padding: 25px;
            box-sizing: border-box;
            overflow-y: auto;
            height: 100%;
            scroll-behavior: smooth;
        }
        #right-panel h5{
            color: lightgray;
            font-size: 24px;
            text-align: center;
        }
        body.alt-color-scheme #right-panel h5{
            color: #00243D;
        }
        .project-title {
            font-size: 60px;
            font-weight: bold;
            text-align: center;
            color: lightgray;
        }
        body.alt-color-scheme .project-title {
            color: #00243D;
        }
        .divider {
            width: 100%;
            height: 2px;
            background-color: lightgray;
            margin-bottom: 30px;
        }
        body.alt-color-scheme .divider {
            background-color: #00243D;
        }
        .categories-container {
            display: flex;
            justify-content: center;
            gap: 20px;
            margin-bottom: 30px;
            text-align: center;
            position: -webkit-sticky; /* For Safari */
            position: sticky;
            top: 0;
            background-color: #151E3D; /* Match the body background color for seamless integration */
            z-index: 1000; /* Ensure it stays above other content */
            padding: 10px 0; /* Add padding for better visual spacing */
            border: 2px solid #ABA9C3; /* Add a border */
            border-radius: 8px; /* Optional: to make the corners rounded */
            box-sizing: border-box; /* Include padding and border in the element's total width and height */
        }
        body.alt-color-scheme .categories-container {
                background-color: #E4EFF1; /* Match the body background color for seamless integration */
                border: 2px solid #00243D; /* Add a border */
        }   
        .category {
            font-size: 24px;
            color: lightgray;
            text-decoration: none;
            padding: 10px 40px;
            border-radius: 5px;
            transition: background-color 0.3s;
        }
        body.alt-color-scheme .category {
            color: #151E3D;
        }
        body.alt-color-scheme .categories-container .category {
            color: black;
        }
        .category:hover {
            background-color: #22365e;
        }
        body.alt-color-scheme .category:hover {
            background-color: #c0c0c0;
        }
        .projects-container {
            display: grid;
            grid-template-columns: repeat(2, 1fr);
            gap: 20px;
        }
        .project {
            background-color: #1b2b4c;
            padding: 20px;
            border-radius: 10px;
            display: flex;
            flex-direction: row;
            align-items: center;
            cursor: pointer;
            transition: transform 0.3s, background-color 0.3s;
        }
        body.alt-color-scheme .project {
            background-color: #e0e0e0;
        }
        .project img {
            width: 30%;
            border-radius: 10px;
            margin-right: 15px;
        }
        .project-title-text {
            font-size: 24px;
            margin-bottom: 10px;
    
        }
        .project-description {
            font-size: 16px;
        }
        .project:hover {
            background-color: #22365e;
            transform: scale(1.05);
        }
        body.alt-color-scheme .project:hover {
            background-color: #c0c0c0;
        }
        .toggle-button {
            position: absolute;
            top: 20px;
            right: 20px;
            background: none;
            border: none;
            cursor: pointer;
            font-size: 24px;
            color: #99a5b1;
        }
        body.alt-color-scheme .toggle-button {
            color: #00243D;
        }
        .back-icon {
            position: absolute;
            top: 20px;
            left: 20px;
            font-size: 24px;
            color: #99a5b1;
            text-decoration: none;
        }
        body.alt-color-scheme .back-icon {
            color: #00243D
        }
        .home-icon {
            position: absolute;
            top: 20px;
            right: 65px;
            font-size: 24px;
            color: #99a5b1;
            text-decoration: none;
        }
        body.alt-color-scheme .home-icon {
            color: #00243D;
        }
        .section {
            margin-bottom: 50px;
            display: flex;
            align-items: flex-start; /* Align items at the start */
            justify-content: space-between; /* Distribute space between items */
        }
        .section .description h2{
            font-size: 36px;
            color: lightgray;
        }
        body.alt-color-scheme .section .description h2{
            color: #151E3D
        }

        .section .description {
            width: 100%; /* Description takes up 70% of the width */
            margin-right: 20px; /* Space between description and image */
        }

        .section .image-container {
            margin-top: 100px;
            width: 40%; /* Image container takes up 25% of the width */
            display: flex;
            flex-direction: column;
            align-items: center; /* Center image and caption */
        }
        .section img, .section video {
            max-width: 100%;
            border-radius: 10px;
            border: 3px solid #99a5b1;
        }
        .section .image-caption {
            font-size: 14px; /* Smaller font for image caption */
            text-align: center;
            margin-top: 10px;
            color: #99a5b1;
        }
        .center-align {
            display: block;
            margin-left: auto;
            margin-right: auto;
        }
        .section .description white{
            color: lightgray;
        }
        body.alt-color-scheme .section .description white{
            color: black;
        }
    </style>
</head>
<body>
    <!-- House Icon -->
    <a href="projects.html" class="back-icon"><i class="fa-solid fa-arrow-left"></i></a>
    <a href="index.html" class="home-icon"><i class="fa-solid fa-house"></i></a>

    
    <button class="toggle-button"><i class="fas fa-moon"></i></button>
    
    <div id="right-panel">
        <div class="project-title">Autonomous Race Car</div>
        <h5><white>Contributors: Arianna Ilvonen, Dora Hu, Jusus Diaz, Charles Ge</white></h5>
        
        <!-- Table of Contents -->
        <div class="categories-container">
            <a href="#overview" class="category">Goal</a>
            <a href="#wallFollowing" class="category">Wall Following</a>
            <a href="#visual" class="category">Computer Vision</a>
            <a href="#localization" class="category">Localization</a>
            <a href="#pathPlanning" class="category">Path Planning</a>
            <a href="#final" class="category">Final Challenge</a>

        </div>
        
        <!-- Sections -->
        <div id="overview" class="section">
            <div class="description">
                <h2>Goal</h2>
                <p style="line-height: 2em;">&emsp; As a part of <a href="https://student.mit.edu/catalog/search.cgi?search=6.4200&style=verbatim&when=*&termleng=4&days_offered=*&start_time=*&duration=*&total_units=*" class="link">6.4200</a>, my team and I were given a racecar robot with an NVIDIA Jetson 
                    Xavier computer, and tasked with programming and executing various operations.
                    Our tasks encompass developing and implementing sophisticated functionalities such as wall-following, a 
                    safety controller, cone detection, parking control, line following, localization, path planning, and path 
                    following. </p>
                <p style="line-height: 2em;">&emsp; To ensure transparency and collaboration, we conducted bi-weekly Lab Briefings and Reports, 
                    during which we discussed the rationale behind our decisions, coding strategies, and collaborative efforts.
                    This project was  a valuable experience, growing my skills in both technical aspects and communication and teamwork.
                </p>
            </div>
            <div class="image-container">
                <img src="./Website_Pics/RSS_candid.png" alt="Inverted Pendulum System">
                <div class="image-caption">MIT 6.4200 Robot Car</div>
            </div>
        </div>
        
        
        <div id="wallFollowing" class="section">
            <div class="description">
                <h2>Wall Following (Sim Done Individually)</h2>
                <figure style="float: right; margin-left: 25px; margin-bottom: 5px; width: 300px; border-color: #B8998E;">
                    <img src="./Website_Pics/LIDAR_Slicing.png" controls class="center-align" width = 400px></img>
                    <figcaption style="text-align: center; font-size: 14px; color: #99a5b1; margin-top: 5px; ">
                        LIDAR sliced into front, left, and right
                    </figcaption>
                </figure>
                <p style="line-height: 2em;">&emsp; The goal of the wall following lab was to write a program that would make it such
                    that the <white><strong>car would drive forward at a set speed and keep a constant distance from either the left or right wall</strong></white>.
                    The car would have to navigate around corners and obstacles. The first step would be to implement code in simulation, and 
                    later develop onto the physical car.
                </p>
                <p style="line-height: 2em;">&emsp; Using the simulator provided by the instructors, I created a ROS node that subscribed to the simulated
                    LIDAR scans, and sliced the scan into a front, left, and right section.
                </p>
                <figure style="float: left; margin-left: 25px; margin-bottom: 5px; width: 400px; border-color: #B8998E;">
                    <img src="./RSS Media/PD_Controller.png" controls class="center-align" width = 400px></img>
                    <figcaption style="text-align: center; font-size: 14px; color: #99a5b1; margin-top: 5px; ">
                        PD Controller Architecture
                    </figcaption>
                </figure>
                <p style="line-height: 2em;">&emsp; Given a wall to follow, I would then calculate the horizontal distance to the wall ( np.sin(Ï´) ), remove any extreme
                    outliers, take the average, and use it as the input to a PD controller. I chose to take the average instead of the median values because upon approaching
                    a corner, the average would push the car to more smoothly turn the corner. I also took the average of the frontal slice, and when that value was below a certain
                    threshold dependent on the velocity, a multiplier would be added to the controller (ensures tight turning radius for faster speeds).
                    Once I tuned the PD controller, I would pushlished an AckermannDrive message with the output of the PD controller 
                    linked to the steering angle of the car.
                </p>
                <figure style="float: right; margin-left: 25px; margin-bottom: 5px; width: 300px; border-color: #B8998E;">
                    <img src="./RSS Media/LR_Line.png" controls class="center-align" width = 400px></img>
                    <figcaption style="text-align: center; font-size: 14px; color: #99a5b1; margin-top: 5px; ">
                        Least Squares Regression with RANSAC applied to reduce noise
                    </figcaption>
                </figure>
                <p style="line-height: 2em;">&emsp; Implementing wall following on the hardware was a little more difficult. Because the data from the actual LIDAR is noisy,
                    simply finding the mean of the normal distance would not be robust enough. Instead, my team and I decided to do a Least Squares Regression on the LIDAR 
                    data to compute a theoretical "wall", and use our normal distance to that wall as the input to our PD controller. 
                </p>
                <figure style="float: left; margin-left: 25px; margin-bottom: 5px; width: 300px; border-color: #B8998E;">
                    <img src="./RSS Media/Braking_Dist.png" controls class="center-align" width = 400px></img>
                    <figcaption style="text-align: center; font-size: 14px; color: #99a5b1; margin-top: 5px; ">
                        Empirical braking distances for varying speeds
                    </figcaption>
                </figure>
                <p style="line-height: 2em;">&emsp; As the hardware is quite expensive, we were also tasked with designing a safety controller for the car. To do this,
                    I wrote a function that, if the average forward distance within the width of the car is under a certain threshold, the function would begin sending
                    an AckermannDrive message of highest priority with its velocity set to 0. The threshold was calculated experimentally by measuring braking distances at different
                    speeds and extracting a minumum braking distance formula. The safety controller would be running at all times during the course of the project.
                </p>
            </div>
        </div>
        
        <div id="visual" class="section">
            <div class="description">
                <h2>Visual Operations</h2>
                <p style="line-height: 2em;">&emsp; This goal of this lab was to learn how to take visual input to allow the <white><strong>
                    racecar to park in front of a cone and perform line following</white></strong>. We dealt with object detection algorithms, homography
                    transformations, and parking controllers.
                </p>
                <p style="line-height: 2em;">&emsp; The first step of this lab was what <white><strong>I was in charge of</white></strong>: detecting a cone given an image.
                </p>
                <p style="line-height: 2em;">&emsp; I first subscribed to a ROS node that published the ZED Camera picture. I then used the <a href="https://opencv.org/" class="link">OpenCV library</a>
                    to apply the series of operations on the image shown below:
                </p>
                <div style="float: center; margin-bottom: 5px; width: 100%; display: flex; flex-wrap: wrap; justify-content: center;">
                    <figure style="margin-bottom: 5px; width: 25%;">
                        <img src="./RSS Media/OG_Img.png" controls class="center-align" width=""></img>
                        <figcaption style="text-align: center; font-size: 14px; color: #99a5b1; margin-top: 5px;">
                            <p style="line-height: 2em;">&emsp;Original Image - No operation</p>
                        </figcaption>
                    </figure>
                
                    <figure style="margin-bottom: 5px; width: 25%;">
                        <img src="./RSS Media/HSV_Img.png" controls class="center-align" width=""></img>
                        <figcaption style="text-align: center; font-size: 14px; color: #99a5b1; margin-top: 5px;">
                            Apply HSV Filter to Image
                        </figcaption>
                    </figure>
                
                    <figure style="margin-bottom: 5px; width: 25%;">
                        <img src="./RSS Media/Red_Img.png" controls class="center-align" width=""></img>
                        <figcaption style="text-align: center; font-size: 14px; color: #99a5b1; margin-top: 5px;">
                            Threshold operation on the red/orange component of the image
                        </figcaption>
                    </figure>
                
                    <figure style="margin-bottom: 5px; width: 25%;">
                        <img src="./RSS Media/Morph_Img.png" controls class="center-align" width=""></img>
                        <figcaption style="text-align: center; font-size: 14px; color: #99a5b1; margin-top: 5px;">
                            MorphOPEN applied - Removes Noise
                        </figcaption>
                    </figure>
                
                    <figure style="margin-bottom: 5px; width: 25%;">
                        <img src="./RSS Media/Simplified_Img.png" controls class="center-align" width=""></img>
                        <figcaption style="text-align: center; font-size: 14px; color: #99a5b1; margin-top: 5px;">
                            Convex-Simplifier Applied
                        </figcaption>
                    </figure>
                
                    <figure style="margin-bottom: 5px; width: 25%;">
                        <img src="./RSS Media/Bounding_Img.png" controls class="center-align" width=""></img>
                        <figcaption style="text-align: center; font-size: 14px; color: #99a5b1; margin-top: 5px;">
                            Bounding Box from Convex Feature
                        </figcaption>
                    </figure>
                </div>
                
                <p style="line-height: 2em;">&emsp; Using the bounding box computed from my node as an input, my teammate <white><strong>Dora Hu</white></strong> 
                    designed a homography transformer that determined the position of the cone relative to the racecar. Using a set of known sample points and 
                    OpenCV libraries, she was able to compute the homography matrix and transform the positions in the pixel frame (u, v) to locations on the ground (x, y). 
                </p>
                <div style="float: right; margin-left: 5px; margin-bottom: 5px; width: 50%; display: flex; flex-direction: row;">
                    <figure style="margin-bottom: 5px;">
                        <img src="./RSS Media/Parking_Logic.png" controls class="center-align" width="100%"></img>
                        <figcaption style="text-align: center; font-size: 14px; color: #99a5b1; margin-top: 5px;">
                            <p style="line-height: 2em;">&emsp;Parking Logic</p>
                        </figcaption>
                    </figure>
                    <figure>
                </div>
                <p style="line-height: 2em;">&emsp; My next teammate <white><strong>Arianna Ilvonen</white></strong> then took this x,y input and designed a parking controller
                    around it, with the logic and an example shown in the table below:
                </p>
                <p style="line-height: 2em;">&emsp; Finally, <white><strong>Charles Ge</white></strong> wrote a line follower that would crop the input image such that a red 
                    stripe of tape would be interpreted as a cone set at a fixed distance ahead - the car will perpetually attempt to reach the cone, but never manage to.
                </p>
                <div style="float: left; margin-left: 5px; margin-bottom: 5px; width: 100%; display: flex; flex-direction: row;">
                    <figure style="margin-bottom: 5px;">
                        <video src="./RSS Media/Park_Vid.mP4" controls class="center-align" width="100%"></video>
                        <figcaption style="text-align: center; font-size: 14px; color: #99a5b1; margin-top: 5px;">
                            <p style="line-height: 2em;">&emsp; Parking Controller in Use</p>
                        </figcaption>
                    </figure>
        
                    <figure>
                        <video src="./RSS Media/Line_Follow.mp4" controls class="center-align" width="100%"></video>
                        <figcaption style="text-align: center; font-size: 14px; color: #99a5b1; margin-top: 5px;">
                            Line Follower Running in a Circle
                        </figcaption>
                    </figure>
                </div>
            </div>
        </div>
        <div id="localization" class="section">
            <div class="description">
                <h2>Localization</h2>
                <p style="line-height: 2em;">&emsp; In this lab, we were tasked with <white><strong>localizing our robot (determining its orientation and position) given a map of our environment by
                    implementing Monte Carlo Localization</strong></white>. My team and I went about this by contructing a motion model and sensor model, and combining this into a particle 
                    filter.
                </p>
                <figure style="float: right; margin-left: 25px; margin-bottom: 5px; width: 250px; border-color: #B8998E;">
                    <img src="./RSS Media/Sensor_Model.png" controls class="center-align" width = 250px></img>
                    <figcaption style="text-align: center; font-size: 14px; color: #99a5b1; margin-top: 5px; ">
                        Empirical braking distances for varying speeds
                    </figcaption>
                </figure>
                <p style="line-height: 2em;">&emsp; To determine where in the map we were, we needed to initialize particles in the approximate area we were in, evaluate their positions
                    and orientations relative to the car's position using LIDAR data, and have the particles converge to one position. This comparison is done using the following equations
                    to find the probability that the current scan is the same as the car's scan:
                    </p>
                <p style="line-height: 2em;">&emsp;
                    One difficult facet of this is the baud rate of the comparison of however many particles you have (usually hundreds) needs to happen very quickly, and this computation
                    needs to be done for each simulated LIDAR value for each particle (approx 10,000 times). Explicitly computing these probabilities then does not become feasible, so <white><strong>
                    Charles Ge and I</strong></white> instead precomputed a lookup table to fetch probabilities from, which increased the update rate from 4Hz to over 20Hz. 
                    After computing each particle's probability, we would converge all particles to the one with the highest probability.
                </p>
                <figure style="float: left; margin-bottom: 5px; width: 25%; border-color: #B8998E;">
                    <video src="./RSS Media/Motion_Model_Odom.mov" controls class="center-align" width = 250px></video>
                    <figcaption style="text-align: center; font-size: 14px; color: #99a5b1; margin-top: 5px; ">
                        Motion Model without noise - perfectly follows the car's odometry
                    </figcaption>
                </figure>
                <figure style="float: left; margin-bottom: 5px; width: 25%; border-color: #B8998E;">
                    <video src="./RSS Media/Motion_Model_Noise.mov" controls class="center-align" width = 250px></video>
                    <figcaption style="text-align: center; font-size: 14px; color: #99a5b1; margin-top: 5px; ">
                        Motion Model Noise and Resampling
                    </figcaption>
                </figure>

                <p style="line-height: 2em;">&emsp;
                    The next phase of the problem was to create the motion model which would update the particle based on the car's odometry. To implement this, <white><strong>Charles and I</strong></white>
                    wrote a file that would subscribe to the car's published odometry topic and update each particle accordingly. To ensure that we are repeatedly converging to the 
                    best possible particle instead of getting an ok particle and sticking with it, we added noise to the odometry so that the sample of particle is constantly changing. 
                </p>
                <p style="line-height: 2em;">&emsp;
                    To bring it all together, we needed to ensure that the motion model would not be updating the particles at the same time as the sensor model was computing the 
                    probabilities. To combat this, <white><strong>Dora Hu </strong></white> implemented threading, to prevent both models from accessing the data concurrently.
                </p>
                <p style="line-height: 2em;">&emsp;
                    And after a couple long nights with the whole team pushing to implement the code onto the hardware, we finally got the localization to work!
                </p>
                <div style="float: left; margin-left: 5px; margin-bottom: 5px; width: 100%; display: flex; flex-direction: row;">
                    <figure style="margin-bottom: 5px;">
                        <video src="./RSS Media/Loc_Sim.mov" controls class="center-align" width="100%"></video>
                        <figcaption style="text-align: center; font-size: 14px; color: #99a5b1; margin-top: 5px;">
                            <p style="line-height: 2em;">&emsp; Localization Working in Simulation</p>
                        </figcaption>
                    </figure>
        
                    <figure>
                        <video src="./RSS Media/Loc_IRL.mov" controls class="center-align" width="100%"></video>
                        <figcaption style="text-align: center; font-size: 14px; color: #99a5b1; margin-top: 5px;">
                            Localization Working on Hardware
                        </figcaption>
                    </figure>
                </div>
            </div>
        </div>
        
        <div id="pathPlanning" class="section">
            <div class="description">
                <h2>Path Planning</h2>
                <p style="line-height: 2em;">&emsp; My team and I were tasked with, given a start and end point, <white><strong>planning trajectories in a known occupancy grid
                    and program our car to drive along this computed path.</strong></white> </p>
                <p style="line-height: 2em;">&emsp;
                    We wanted to implement various different types of path planners to test which one would be most effective, and thus get the fastest computation or shortest path. 
                    Therefore, <white><strong>Jesus</strong></white> worked on a BFS path planner, <white><strong>I</strong></white> worked on an A* path planner, and 
                    <white><strong>Arianna</strong></white> worked on an RRT* path planner. 
                </p>
                <p style="line-height: 2em;">&emsp;
                    To implement A*, I first discritized the map into 0.5m x 0.5m squares and assigned occupancy values to the squares (1 if the square was occupiable, 0 if it was not). 
                    For my total cost function, I used the following:
                </p>
                <div style="float: left; margin-left: 5px; margin-bottom: 5px; width: 100%; display: flex; flex-direction: row;">
                    <figure style="margin-bottom: 5px;">
                        <img src="./RSS Media/A*1.png" controls class="center-align" width="70%"></img>
                        <figcaption style="text-align: center; font-size: 14px; color: #99a5b1; margin-top: 5px;">
                            <p style="line-height: 2em;">&emsp; A-star Step 1: Initial</p>
                        </figcaption>
                    </figure>
        
                    <figure>
                        <img src="./RSS Media/A*2.png" controls class="center-align" width="70%"></img>
                        <figcaption style="text-align: center; font-size: 14px; color: #99a5b1; margin-top: 5px;">
                            <p style="line-height: 2em;">&emsp; A-star Step 2: Calculate Costs</p>
                        </figcaption>
                    </figure>

                    <figure>
                        <img src="./RSS Media/A*3.png" controls class="center-align" width="70%"></img>
                        <figcaption style="text-align: center; font-size: 14px; color: #99a5b1; margin-top: 5px;">
                            <p style="line-height: 2em;">&emsp; A-star Step 3: Choose Cheapest Step</p>
                        </figcaption>
                    </figure>

                    <figure>
                        <img src="./RSS Media/A*4.png" controls class="center-align" width="70%"></img>
                        <figcaption style="text-align: center; font-size: 14px; color: #99a5b1; margin-top: 5px;">
                            <p style="line-height: 2em;">&emsp;  A-star Step 4: Calculate Cost About New Square</p>
                        </figcaption>
                    </figure>

                    <figure>
                        <img src="./RSS Media/A*5.png" controls class="center-align" width="70%"></img>
                        <figcaption style="text-align: center; font-size: 14px; color: #99a5b1; margin-top: 5px;">
                            <p style="line-height: 2em;">&emsp; A-star Step 5: Reach Goal</p>
                        </figcaption>
                    </figure>
                </div>
                <figure style="float: right; margin-bottom: 5px; width: 30%; border-color: #B8998E;">
                    <img src="./RSS Media/Path_Plan_All.png" controls class="center-align" width = 100%></img>
                    <figcaption style="text-align: center; font-size: 14px; color: #99a5b1; margin-top: 5px; ">
                        Motion Model Noise and Resampling
                    </figcaption>
                </figure>
                <p style="line-height: 2em;">&emsp;
                    Both A* and BFS are optimal pathfinding algorithms - BFS is more naive, but was ultimately much faster due to the small size of the map and the computation time of A*. 
                    RRT* is a sampling based algorithm and evolves over time to get better solutions, approaching optimal as time goes to infinity. The computation time, path lengths, and 
                    final paths are visualized below for all three methods.
                </p>             
                <p style="line-height: 2em;">&emsp;
                    Once the trajecories were computed, <white><strong>Charles and Dora</strong></white> worked on implementing a pure pursuit controller for our robot. The way our controller worked,
                    is that we would first compute distances to each line segment in the trajectory and find the closest one. A target point at a specified lookahead radius on the trajectory would 
                    then be used to compute the steering input for the car. 
                </p>
                <p style="line-height: 2em;">&emsp;
                    Wrapping the localization in with the trajectory planner and pure pursuit controller, we were able to implement the code onto the hardware and got the following
                    results:
                </p>
                <figure style="float: center; margin-bottom: 5px; width: 60%; border-color: #B8998E;">
                    <video src="./RSS Media/Path_Plan_Vid.mov" controls class="center-align" width = 100%></video>
                    <figcaption style="text-align: center; font-size: 14px; color: #99a5b1; margin-top: 5px; ">
                        Motion Model Noise and Resampling
                    </figcaption>
                </figure>
            </div>
        </div>

        <div id="final" class="section">
            <div class="description">
                <h2>Final Challenge</h2>
                <figure style="float: left; margin-bottom: 5px; width: 18%; border-color: #B8998E;">
                    <video src="./RSS Media/Johnson_Run.mov" controls class="center-align" width = 100%></video>
                    <figcaption style="text-align: center; font-size: 14px; color: #99a5b1; margin-top: 5px; ">
                        Robot racing around Johnson Track 
                    </figcaption>
                </figure>
                <figure style="float: right; margin-bottom: 5px; width: 18%; border-color: #B8998E;">
                    <video src="./RSS Media/Luigi_Run.mov" controls class="center-align" width = 100%></video>
                    <figcaption style="text-align: center; font-size: 14px; color: #99a5b1; margin-top: 5px; ">
                        Robot navigating the City
                    </figcaption>
                </figure>
                <p style="line-height: 2em;">&emsp; The final challenge was meant to encompass everything we had implemented so far. There were two parts to the challenge: 
                    <white><strong>a time trial around a track and a city-driving component</strong></white>. We needed to race around the track within a certain time 
                    window, and navigate around a constructed city, obeying traffic laws, lanes, and avoiding pedestrians while stopping at three seperate locations.
                </p>
                <p style="line-height: 2em;">&emsp; To navigate around the track, we used the OpenCV library to detect the lanes (penalties applied
                    for crossing lanes), and performed a series of operations to extract a goal point at a certain lookahead.
                </p>
                <p style="line-height: 2em;">&emsp;  As the car would run, the image would update, which meant I could feed this input into a PD Controller and tune it to stabilize in the center of the lane.
                </p>
                <p style="line-height: 2em;">&emsp; For the city navigation, <white><strong>Jesus</strong></white> developed a bidirectional version of his BFS algorithm to
                    obey the direction of travel (drive only in the right lane) and implement U-turns, and we stiched together the trajectories from the three stopping points 
                    to get one long trajectory to follow. 
                </p>
                <p style="line-height: 2em;">&emsp; To obey the traffic lights and stop signs, we again used OpenCV to detect stop signs and red light, and to stop for the appropriate 
                    amounts of time. Ultimately, the long uniform hallway that we had to navigate midway through our trajectory was too nondescript for out localizer to work effectively
                    and we were only able to achieve the first two stops before clipping a wall.
                </p>
            </div>
        </div>
    </div>
    
    <script>
        const toggleButton = document.querySelector('.toggle-button');
        toggleButton.addEventListener('click', () => {
            document.body.classList.toggle('alt-color-scheme');
            const icon = toggleButton.querySelector('i');
            if (document.body.classList.contains('alt-color-scheme')) {
                icon.classList.remove('fa-moon');
                icon.classList.add('fa-sun');
            } else {
                icon.classList.remove('fa-sun');
                icon.classList.add('fa-moon');
            }
        });
    </script>
</body>
</html>
